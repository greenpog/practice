{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greenpog/practice/blob/develop/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Лабораторная работа №2 Парсинг"
      ],
      "metadata": {
        "id": "kHV7Tz-MHjNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Теоретический материал к Лабораторной работе №2**\n",
        "\n",
        "**Тема: Основы автоматизированного сбора данных из веб-источников**\n",
        "\n",
        "#### **Введение: от веб-страницы к структурированным данным**\n",
        "\n",
        "Современный Интернет представляет собой крупнейший в истории человечества источник информации. Однако эти данные, как правило, представлены в неструктурированном, человекочитаемом формате — в виде веб-страниц. Процесс автоматического извлечения данных с веб-сайтов и их преобразования в структурированный, машиночитаемый вид (например, в таблицу или базу данных) получил название **веб-парсинг** (от англ. *to parse* — анализировать, разбирать) или **веб-скрейпинг** (*to scrape* — соскребать).\n",
        "\n",
        "Для выполнения этой задачи мы будем использовать экосистему из нескольких специализированных библиотек Python, каждая из которых выполняет свою строго определенную функцию. В данной работе мы сосредоточимся на двух основных инструментах для работы со статичными сайтами.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Библиотека `requests`: Протокол взаимодействия с веб-сервером**\n",
        "\n",
        "Любое взаимодействие в сети Интернет начинается с отправки запроса. Когда вы вводите адрес сайта в браузере, он отправляет HTTP-запрос к серверу, на котором этот сайт расположен. Сервер в ответ присылает HTML-документ, который браузер и отображает.\n",
        "\n",
        "Библиотека `requests` является отраслевым стандартом в Python для выполнения этой задачи программным путем. Её основная функция — абстрагироваться от сложностей сетевых протоколов и предоставить простой интерфейс для отправки HTTP-запросов.\n",
        "\n",
        "**Ключевые концепции и синтаксис:**\n",
        "\n",
        "1.  **Отправка GET-запроса:** Основной метод, который мы используем, — `requests.get()`. Он эмулирует переход по URL-адресу в браузере.\n",
        "\n",
        "    ```python\n",
        "    import requests\n",
        "\n",
        "    # URL-адрес целевого ресурса\n",
        "    url = 'http://quotes.toscrape.com/'\n",
        "\n",
        "    # Отправка запроса. Вся информация об ответе сервера будет храниться в объекте 'response'\n",
        "    response = requests.get(url)\n",
        "    ```\n",
        "\n",
        "2.  **Объект ответа (`response`):** Результатом вызова `requests.get()` является объект, содержащий всю информацию об ответе сервера. Наиболее важные для нас атрибуты:\n",
        "    *   `response.status_code`: Числовой код состояния HTTP. Успешный запрос возвращает код **200**. Коды, начинающиеся с 4 (например, 404 Not Found) или 5 (например, 500 Internal Server Error), свидетельствуют об ошибках. Проверка этого кода — обязательный шаг для написания надежного парсера.\n",
        "    *   `response.text`: Содержимое ответа сервера в виде текстовой строки. В нашем случае это будет полный HTML-код запрошенной страницы.\n",
        "\n",
        "    **Пример использования:**\n",
        "\n",
        "    ```python\n",
        "    if response.status_code == 200:\n",
        "        print(\"Запрос выполнен успешно.\")\n",
        "        # Получаем HTML-код страницы\n",
        "        html_content = response.text\n",
        "        print(\"Длина полученного HTML-документа:\", len(html_content), \"символов.\")\n",
        "    else:\n",
        "        print(\"Произошла ошибка при запросе. Код:\", response.status_code)\n",
        "    ```\n",
        "\n",
        "На данном этапе `requests` свою задачу выполнил: мы получили \"сырой\" HTML-документ. Далее его необходимо проанализировать.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Библиотека `BeautifulSoup`: Навигация по DOM-структуре документа**\n",
        "\n",
        "HTML-документ — это не просто текст, а строго иерархическая структура, описываемая с помощью тегов. Эту структуру принято называть **DOM-деревом** (Document Object Model). Библиотека `BeautifulSoup` является мощнейшим инструментом для парсинга этого дерева. Она преобразует текстовую строку HTML в объектную модель, по которой можно осуществлять удобную навигацию и поиск.\n",
        "\n",
        "**Ключевые концепции и синтаксис:**\n",
        "\n",
        "1.  **Инициализация объекта (\"создание супа\"):** Первым шагом является создание экземпляра класса `BeautifulSoup`, который принимает на вход HTML-текст и название парсера.\n",
        "\n",
        "    ```python\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    # html_content - это строка, полученная от requests.text\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    ```\n",
        "\n",
        "2.  **Поиск элементов:** `BeautifulSoup` предоставляет два основных метода для поиска тегов внутри DOM-дерева. Они используют **селекторы** — комбинации имени тега и его атрибутов (например, `class` или `id`).\n",
        "\n",
        "    *   **`soup.find(tag, attributes)`**: Ищет **первый** элемент, соответствующий заданным критериям, и возвращает его как объект тега. Если ничего не найдено, возвращает `None`.\n",
        "\n",
        "      **Синтаксис:**\n",
        "      ```python\n",
        "      # Поиск первого тега <h1>\n",
        "      first_h1 = soup.find('h1')\n",
        "\n",
        "      # Поиск первого тега <span> с атрибутом class='text'\n",
        "      # ВАЖНО: 'class' - зарезервированное слово в Python, поэтому используется аргумент 'class_'\n",
        "      first_quote_text = soup.find('span', class_='text')\n",
        "      ```\n",
        "\n",
        "    *   **`soup.find_all(tag, attributes)`**: Ищет **все** элементы, соответствующие критериям, и возвращает их в виде списка (`list`). Если ничего не найдено, возвращает пустой список.\n",
        "\n",
        "      **Синтаксис:**\n",
        "      ```python\n",
        "      # Поиск всех тегов <div> с атрибутом class='quote'\n",
        "      all_quote_containers = soup.find_all('div', class_='quote')\n",
        "\n",
        "      # Итерация по результатам\n",
        "      for container in all_quote_containers:\n",
        "          # Внутри каждого найденного контейнера можно продолжать поиск\n",
        "          author = container.find('small', class_='author')\n",
        "          print(author.text)\n",
        "      ```\n",
        "\n",
        "3.  **Извлечение содержимого из найденных тегов:** После того как тег найден, из него можно извлечь полезную информацию.\n",
        "    *   **`.text`**: Возвращает все текстовое содержимое внутри тега и его дочерних элементов в виде одной строки.\n",
        "    *   **`tag['attribute_name']`**: Позволяет получить значение конкретного атрибута тега. Чаще всего используется для извлечения ссылок из атрибута `href` у тега `<a>`.\n",
        "\n",
        "      **Пример использования:**\n",
        "      ```python\n",
        "      # Найдем тег с цитатой\n",
        "      quote_element = soup.find('div', class_='quote')\n",
        "\n",
        "      # Извлекаем текст цитаты\n",
        "      text = quote_element.find('span', class_='text').text\n",
        "      print(\"Текст цитаты:\", text)\n",
        "\n",
        "      # Извлекаем ссылку на автора (если она есть)\n",
        "      author_link = quote_element.find('a') # Находим первый тег <a> внутри контейнера\n",
        "      if author_link:\n",
        "          href_value = author_link['href']\n",
        "          print(\"Ссылка на страницу автора:\", href_value)\n",
        "      ```\n",
        "\n",
        "\n",
        "\n",
        "Рассмотренный ранее подход с использованием библиотек `requests` и `BeautifulSoup` является высокоэффективным для работы со **статичными** веб-страницами. \"Статичная\" страница — это документ, HTML-код которого полностью формируется на сервере и доставляется клиенту в готовом виде. Однако значительная часть современного веба функционирует иначе.\n",
        "\n",
        "**Динамические веб-сайты** активно используют технологию JavaScript для модификации своего содержимого непосредственно в браузере пользователя *после* первоначальной загрузки страницы. Это может быть подгрузка новостной ленты при прокрутке, отображение цен на авиабилеты после выбора маршрута, обновление графика погоды в реальном времени.\n",
        "\n",
        "При попытке парсинга таких сайтов с помощью `requests`, мы получим лишь базовый HTML-шаблон, в котором искомые данные будут отсутствовать, поскольку JavaScript-код, ответственный за их загрузку и отображение, не будет исполнен.\n",
        "\n",
        "Для решения этой фундаментальной проблемы необходим инструмент, который не просто запрашивает HTML, а эмулирует поведение полноценного веб-браузера. Таким инструментом является библиотека **Selenium**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Парадигма Selenium: Управление браузером вместо отправки запросов**\n",
        "\n",
        "Основное отличие Selenium от `requests` заключается в подходе. Если `requests` — это \"курьер\", доставляющий HTML-документ, то **Selenium — это \"робот-пользователь\"**, который программно запускает и управляет реальным браузером (Google Chrome, Firefox и др.).\n",
        "\n",
        "Этот подход позволяет:\n",
        "*   **Исполнять JavaScript:** Браузер под управлением Selenium загружает и выполняет все скрипты на странице.\n",
        "*   **Взаимодействовать с элементами:** Selenium может эмулировать действия пользователя, такие как клики по кнопкам, ввод текста в поля, прокрутку страницы.\n",
        "*   **Работать с итоговым HTML:** После всех динамических модификаций мы получаем доступ к финальному, \"отрисованному\" DOM-дереву, которое и видит пользователь.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Ключевые компоненты и синтаксис Selenium**\n",
        "\n",
        "##### **2.1. WebDriver: Мост между кодом и браузером**\n",
        "\n",
        "Центральным элементом Selenium является **WebDriver**. Это программный интерфейс (API), который выступает в роли \"драйвера\" или \"переводчика\" между командами в вашем Python-скрипте и действиями в реальном приложении браузера.\n",
        "\n",
        "**Инициализация WebDriver:**\n",
        "Для начала работы необходимо создать экземпляр WebDriver для конкретного браузера. Современные версии Selenium (`4.6.0` и новее) автоматически управляют загрузкой необходимого драйвера.\n",
        "\n",
        "```python\n",
        "from selenium import webdriver\n",
        "\n",
        "# Инициализация драйвера для Google Chrome.\n",
        "# Selenium сам скачает и настроит chromedriver.\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Команда ниже откроет окно браузера Chrome\n",
        "```\n",
        "\n",
        "##### **2.2. Навигация и получение страницы**\n",
        "Основной метод для загрузки страницы — `driver.get(url)`.\n",
        "\n",
        "```python\n",
        "url = 'https://www.gismeteo.ru/weather-moscow-4368/'\n",
        "driver.get(url) # Браузер откроется и перейдет по указанному адресу\n",
        "```\n",
        "\n",
        "##### **2.3. Проблема асинхронности и механизмы ожидания**\n",
        "\n",
        "Это **самая важная и сложная концепция** при работе с Selenium. Ваш Python-скрипт выполняется гораздо быстрее, чем браузер успевает загрузить страницу и выполнить все JavaScript-команды. Если вы попытаетесь найти элемент сразу после вызова `driver.get()`, скорее всего, вы получите ошибку `NoSuchElementException`, потому что элемент еще не появился на странице.\n",
        "\n",
        "**Неправильный подход:** `time.sleep(5)`. Использование жестких пауз — плохая практика. Пауза может быть слишком короткой (данные не успеют загрузиться) или слишком длинной (скрипт будет работать неэффективно).\n",
        "\n",
        "**Правильный подход: Явные ожидания (Explicit Waits)**\n",
        "Это механизм, который заставляет WebDriver ждать наступления определенного события (например, появления элемента) в течение заданного максимального времени.\n",
        "\n",
        "**Синтаксис:**\n",
        "```python\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Создаем объект ожидания: ждать максимум 10 секунд\n",
        "wait = WebDriverWait(driver, 10)\n",
        "\n",
        "# Команда \"ждать, пока элемент с указанным локатором не станет присутствовать в DOM\"\n",
        "# By.CLASS_NAME — это способ поиска (локатор)\n",
        "# 'unit_temperature_c' — значение локатора\n",
        "temperature_element = wait.until(\n",
        "    EC.presence_of_element_located((By.CLASS_NAME, 'unit_temperature_c'))\n",
        ")\n",
        "```\n",
        "\n",
        "##### **2.4. Поиск элементов и взаимодействие с ними**\n",
        "\n",
        "Для поиска элементов Selenium использует **локаторы**, которые указывают, *как* именно искать элемент. Они импортируются из `selenium.webdriver.common.by.By`.\n",
        "\n",
        "Основные локаторы:\n",
        "*   `By.ID`\n",
        "*   `By.CLASS_NAME`\n",
        "*   `By.TAG_NAME`\n",
        "*   `By.XPATH` (самый мощный и сложный)\n",
        "*   `By.CSS_SELECTOR` (часто самый удобный)\n",
        "\n",
        "**Методы поиска:**\n",
        "*   `driver.find_element(By.ЛОКАТОР, 'значение')`: Ищет **первый** элемент.\n",
        "*   `driver.find_elements(By.ЛОКАТОР, 'значение')`: Ищет **все** элементы и возвращает список.\n",
        "\n",
        "**Методы взаимодействия:**\n",
        "*   `.click()`: Кликнуть по элементу.\n",
        "*   `.send_keys('текст')`: Ввести текст в поле ввода.\n",
        "*   `.text`: Получить видимый текст элемента.\n",
        "*   `.get_attribute('атрибут')`: Получить значение атрибута (например, `href`).\n",
        "\n",
        "**Пример:**\n",
        "```python\n",
        "# Найти поле поиска по его ID\n",
        "search_box = driver.find_element(By.ID, 'search-input')\n",
        "\n",
        "# Ввести текст в поле\n",
        "search_box.send_keys('Погода в Санкт-Петербурге')\n",
        "\n",
        "# Найти и кликнуть по кнопке поиска\n",
        "search_button = driver.find_element(By.CLASS_NAME, 'search-button')\n",
        "search_button.click()\n",
        "```\n",
        "\n",
        "##### **2.5. Интеграция с BeautifulSoup и завершение работы**\n",
        "\n",
        "После того как Selenium выполнил все необходимые действия (клики, прокрутку) и дождался появления данных, мы можем получить итоговый HTML-код страницы.\n",
        "\n",
        "*   `driver.page_source`: Атрибут, содержащий финальный HTML-код страницы в виде строки.\n",
        "\n",
        "Этот код можно передать в `BeautifulSoup` для более удобного и быстрого парсинга, комбинируя сильные стороны обеих библиотек.\n",
        "\n",
        "**Обязательный шаг: Завершение сессии**\n",
        "После окончания работы необходимо закрыть браузер и завершить сессию WebDriver, чтобы освободить системные ресурсы.\n",
        "\n",
        "*   `driver.quit()`: Закрывает все окна браузера и завершает процесс WebDriver.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K4E9fULxHozb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requests"
      ],
      "metadata": {
        "id": "VuF-Ty7YmGCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json # Библиотека для работы с JSON\n",
        "\n",
        "# --- ШАГ 1: Определение цели ---\n",
        "# URL API для получения репозиториев пользователя.\n",
        "# Обратите внимание, что это не обычный URL для браузера!\n",
        "api_base_url = \"https://api.github.com/users/\"\n",
        "username = \"gvanrossum\"\n",
        "full_api_url = f\"{api_base_url}{username}/repos\"\n",
        "\n",
        "# --- ШАГ 2: Использование параметров запроса (params) ---\n",
        "# API позволяет настраивать вывод. Мы хотим отсортировать репозитории\n",
        "# по дате создания ('created') и получать по 10 штук за раз.\n",
        "# Для этого используются GET-параметры, которые requests умеет добавлять к URL.\n",
        "\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Создайте словарь 'params', который будет содержать следующие GET-параметры:\n",
        "# 1. 'sort': со значением 'created' (сортировка по дате создания)\n",
        "# 2. 'per_page': со значением '10' (выводить по 10 репозиториев на странице)\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "params = {\n",
        "    '???': 'created',\n",
        "    '???': '10'\n",
        "}\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "\n",
        "# --- ШАГ 3: Выполнение запроса и обработка JSON ---\n",
        "print(f\"Отправляю запрос на: {full_api_url}\")\n",
        "response = requests.get(full_api_url, params=params)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\"Запрос успешен!\")\n",
        "    # --- ЗАДАНИЕ ---\n",
        "    # Ответ от API приходит в формате JSON.\n",
        "    # У объекта response есть специальный метод .json(), который\n",
        "    # автоматически преобразует этот ответ в python-объект (список словарей).\n",
        "    # Используйте его и сохраните результат в переменную 'repos_data'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    repos_data = response.???()\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # --- ШАГ 4: Вывод результатов ---\n",
        "    print(f\"Последние 10 созданных репозиториев пользователя {username}:\")\n",
        "    # Пройдемся циклом по списку репозиториев и выведем их названия и URL\n",
        "    for repo in repos_data:\n",
        "        print(f\"  - Название: {repo['name']}, URL: {repo['html_url']}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Ошибка! Статус-код: {response.status_code}\")\n",
        "    print(f\"Сообщение: {response.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "RUxZBQrWLKEi",
        "outputId": "a54ca288-859c-4227-a4bf-c90f49be5b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-210946018.py, line 40)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-210946018.py\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    repos_data = response.???()\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from IPython.display import Image # Для отображения картинки в Colab\n",
        "\n",
        "# --- ШАГ 1: Найти URL изображения ---\n",
        "# Сначала нам нужно, как в прошлой лабораторной, найти ссылку на логотип.\n",
        "base_url = 'http://quotes.toscrape.com/'\n",
        "response = requests.get(base_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Логотип находится в теге <img> внутри тега <a> с href=\"/\".\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Найдите тег 'img' и извлеките из него значение атрибута 'src'.\n",
        "# Сохраните относительный URL в переменную 'relative_logo_url'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "logo_element = soup.find('a', href='/').find('???')\n",
        "relative_logo_url = logo_element['???']\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "# --- ШАГ 2: Преобразование относительного URL в абсолютный ---\n",
        "# Ссылка в 'src' относительная ('/images/logo.png').\n",
        "# Чтобы ее скачать, нужен полный URL.\n",
        "absolute_logo_url = urljoin(base_url, relative_logo_url)\n",
        "print(f\"Найден абсолютный URL логотипа: {absolute_logo_url}\")\n",
        "\n",
        "\n",
        "# --- ШАГ 3: Скачивание бинарного контента ---\n",
        "print(\"Скачиваю изображение...\")\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Отправьте GET-запрос на 'absolute_logo_url'.\n",
        "# Ответ для бинарных файлов нужно получать через атрибут .content, а не .text\n",
        "# Сохраните результат в переменную 'image_content'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "image_response = requests.get(???)\n",
        "if image_response.status_code == 200:\n",
        "    image_content = image_response.???\n",
        "    print(\"Изображение успешно скачано!\")\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # --- ШАГ 4: Сохранение файла на диск ---\n",
        "    # Используем стандартный синтаксис Python для записи файлов.\n",
        "    # 'wb' означает \"write binary\" - запись в бинарном режиме.\n",
        "    file_name = 'logo.png'\n",
        "    # --- ЗАДАНИЕ ---\n",
        "    # Откройте файл 'file_name' для записи в бинарном режиме ('wb')\n",
        "    # и запишите в него 'image_content'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    with open(???, '???') as f:\n",
        "        f.write(???)\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "    print(f\"Файл '{file_name}' сохранен в текущую директорию Colab.\")\n",
        "\n",
        "    # Отобразим скачанное изображение прямо в блокноте\n",
        "    display(Image(file_name))\n",
        "else:\n",
        "    print(f\"Ошибка при скачивании изображения! Статус-код: {image_response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "jGRj_FXxLMwq",
        "outputId": "74b446f3-88bd-4552-95bd-2aadf23a4cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2772517807.py, line 35)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2772517807.py\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    image_response = requests.get(???)\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- ШАГ 1: Создание сессии и получение CSRF-токена ---\n",
        "# requests.Session() - это объект, который будет \"помнить\" cookies между запросами.\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Создайте объект сессии и сохраните его в переменную 'session'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "session = requests.???()\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "login_url = 'http://quotes.toscrape.com/login'\n",
        "\n",
        "# Сначала делаем GET-запрос, чтобы получить страницу входа и специальный\n",
        "# \"csrf_token\" - это защита от межсайтовой подделки запроса.\n",
        "response_login_page = session.get(login_url)\n",
        "soup_login = BeautifulSoup(response_login_page.text, 'html.parser')\n",
        "\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Найдите тег 'input' у которого атрибут name равен 'csrf_token'\n",
        "# и извлеките из него значение атрибута 'value'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "csrf_token = soup_login.find('input', {'name': '???'})['???']\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "print(f\"Получен CSRF токен: {csrf_token}\")\n",
        "\n",
        "\n",
        "# --- ШАГ 2: Подготовка данных для POST-запроса ---\n",
        "# Это данные, которые мы бы ввели в форму на сайте.\n",
        "# Имена полей ('username', 'password') нужно посмотреть в HTML-коде страницы.\n",
        "payload = {\n",
        "    'csrf_token': csrf_token,\n",
        "    'username': 'admin',  # Используем стандартные учетные данные для этого сайта\n",
        "    'password': 'admin'\n",
        "}\n",
        "\n",
        "\n",
        "# --- ШАГ 3: Отправка POST-запроса для аутентификации ---\n",
        "# Мы отправляем POST-запрос на тот же URL, но уже с нашими данными.\n",
        "# Сессия автоматически сохранит cookies, которые вернет сервер после успешного входа.\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Отправьте POST-запрос с помощью объекта 'session'.\n",
        "# URL: login_url\n",
        "# Данные: payload\n",
        "# Сохраните ответ в 'response_post'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "response_post = session.???(???, data=???)\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "\n",
        "# --- ШАГ 4: Доступ к защищенной странице ---\n",
        "# Теперь, используя ту же сессию, мы можем зайти на любую страницу сайта,\n",
        "# и сервер будет \"видеть\" нас как залогиненного пользователя.\n",
        "print(\"\\nПробую получить доступ к главной странице после логина...\")\n",
        "response_main_page = session.get('http://quotes.toscrape.com/')\n",
        "soup_main = BeautifulSoup(response_main_page.text, 'html.parser')\n",
        "\n",
        "# Проверим, видим ли мы кнопку \"Logout\"\n",
        "logout_button = soup_main.find('a', href='/logout')\n",
        "\n",
        "if logout_button:\n",
        "    print(\"Успех! Мы авторизованы. Сервер видит кнопку 'Logout'.\")\n",
        "    print(logout_button.text)\n",
        "else:\n",
        "    print(\"Неудача. Авторизация не удалась, кнопка 'Logout' не найдена.\")"
      ],
      "metadata": {
        "id": "U80X12wQLOwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NjoG6qGmmJkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BeautifulSoup4"
      ],
      "metadata": {
        "id": "a2FxGZ6QmLdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# --- ШАГ 1: Получение HTML ---\n",
        "url = 'http://books.toscrape.com/'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# --- ШАГ 2: Поиск общего контейнера ---\n",
        "# Все книги находятся внутри элементов <article> с классом 'product_pod'.\n",
        "# --- ЗАДАНИЕ ---\n",
        "# Найдите ВСЕ такие контейнеры с помощью .find_all() и сохраните в 'all_books'.\n",
        "# ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "all_books = soup.find_all('???', class_='???')\n",
        "# ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "print(f\"Найдено книг на странице: {len(all_books)}\")\n",
        "books_data = []\n",
        "\n",
        "# --- ШАГ 3: Извлечение данных в цикле ---\n",
        "for book in all_books:\n",
        "    # --- ЗАДАНИЕ A: Найти название книги ---\n",
        "    # Название находится в теге <a> внутри тега <h3>.\n",
        "    # Нужно извлечь его атрибут 'title'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    title = book.find('???').find('???')['???']\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # --- ЗАДАНИЕ B: Найти цену ---\n",
        "    # Цена находится в теге <p> с классом 'price_color'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    price = book.find('???', class_='???').text\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # --- ЗАДАНИЕ C: Найти рейтинг ---\n",
        "    # Рейтинг находится в атрибуте 'class' у тега <p>, который начинается с 'star-rating'.\n",
        "    # Например: <p class=\"star-rating Three\">. Нам нужно извлечь слово 'Three'.\n",
        "    # ▼▼▼ ВАШ КОД ЗДЕСЬ ▼▼▼\n",
        "    rating_container = book.find('p', class_='star-rating')\n",
        "    # Атрибут 'class' возвращает список классов, например, ['star-rating', 'Three']\n",
        "    rating = rating_container['???'][1] # Берем второй элемент\n",
        "    # ▲▲▲ ВАШ КОД ЗДЕСЬ ▲▲▲\n",
        "\n",
        "    # Добавляем собранные данные в наш список\n",
        "    books_data.append({\n",
        "        'Title': title,\n",
        "        'Price': price,\n",
        "        'Rating': rating\n",
        "    })\n",
        "\n",
        "# --- ШАГ 4: Вывод результата ---\n",
        "df = pd.DataFrame(books_data)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "BBNhpZgPMGUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Этап 1: Импорт библиотек ---\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# --- Этап 2: Подготовка к запросу ---\n",
        "\n",
        "# URL страницы, с которой будем работать\n",
        "url = 'https://ru.wikipedia.org/wiki/Python'\n",
        "\n",
        "# Заголовки для имитации запроса от браузера. Это важная часть!\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# --- Этап 3: Получение и разбор HTML ---\n",
        "try:\n",
        "    print(f\"Отправляем запрос на: {url}\")\n",
        "    # 1. ВАШ КОД ЗДЕСЬ: Отправьте GET-запрос на 'url' с использованием 'headers'.\n",
        "    #    Результат сохраните в переменную 'response'.\n",
        "    #    Подсказка: используйте requests.get(...)\n",
        "    response = ...\n",
        "\n",
        "    # Эта строка проверит, успешно ли выполнен запрос. Если нет, она вызовет ошибку.\n",
        "    response.raise_for_status()\n",
        "    print(\"Ответ от сервера успешно получен!\")\n",
        "\n",
        "    # 2. ВАШ КОД ЗДЕСЬ: Создайте объект BeautifulSoup для парсинга HTML.\n",
        "    #    Передайте ему текст ответа (response.text) и укажите парсер 'lxml'.\n",
        "    #    Результат сохраните в переменную 'soup'.\n",
        "    soup = ...\n",
        "\n",
        "    # --- Этап 4: Поиск нужных элементов ---\n",
        "\n",
        "    # 3. ВАШ КОД ЗДЕСЬ: Найдите ВСЕ теги 'table', у которых атрибут class равен 'wikitable'.\n",
        "    #    Результат (список найденных таблиц) сохраните в переменную 'wikitables'.\n",
        "    #    Подсказка: используйте метод soup.find_all(...) и параметр class_ (с нижним подчеркиванием!).\n",
        "    wikitables = ...\n",
        "\n",
        "    print(f\"Найдено таблиц с классом 'wikitable': {len(wikitables)}\")\n",
        "\n",
        "    # --- Этап 5: Обработка и вывод результатов ---\n",
        "    if not wikitables:\n",
        "        print(\"Таблицы не найдены. Проверьте правильность тега и класса.\")\n",
        "    else:\n",
        "        # Проходим циклом по списку найденных HTML-таблиц\n",
        "        for i, table in enumerate(wikitables):\n",
        "\n",
        "            # 4. ВАШ КОД ЗДЕСЬ: Преобразуйте HTML-код таблицы в DataFrame.\n",
        "            #    Подсказка: используйте pd.read_html(). Ей нужно передать таблицу в виде строки (str(table)).\n",
        "            #    Помните, что pd.read_html() всегда возвращает СПИСОК, поэтому возьмите первый элемент [0].\n",
        "            df = ...\n",
        "\n",
        "            # Выводим результат\n",
        "            print(f\"\\n---------- Таблица №{i+1} ----------\")\n",
        "            print(df.head()) # .head() выведет только первые 5 строк для краткости\n",
        "\n",
        "# --- Этап 6: Обработка ошибок ---\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Ошибка! Не удалось получить страницу. Причина: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Произошла непредвиденная ошибка: {e}\")"
      ],
      "metadata": {
        "id": "SLqZjPdxY8gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "SmYV2Vwj7hIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selenium"
      ],
      "metadata": {
        "id": "OvMnn8-LmPtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Настройка selenium"
      ],
      "metadata": {
        "id": "9RER97g1NSEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Устанавливаем библиотеку selenium для управления браузером\n",
        "!pip install selenium\n",
        "\n",
        "# Устанавливаем браузер chromium и его драйвер\n",
        "# Опция -y автоматически отвечает \"yes\" на все запросы в процессе установки\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-browser chromium-chromedriver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QeB-ElFkbM6p",
        "outputId": "7888659f-a890-487f-cbf2-f54b12977966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.36.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio<1.0,>=0.30.0 (from selenium)\n",
            "  Downloading trio-0.31.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.10.5)\n",
            "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (4.15.0)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio<1.0,>=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.36.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.31.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.7/512.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.36.0 trio-0.31.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:6 https://cli.github.com/packages stable InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,751 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,584 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,276 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,425 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,077 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.8 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,352 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,811 kB]\n",
            "Fetched 24.7 MB in 4s (6,356 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor libfuse3-3 libudev1 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 34.2 MB of archives.\n",
            "After this operation, 134 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.17 [76.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.17 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.71+ubuntu22.04 [31.6 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.6 [3,668 B]\n",
            "Fetched 34.2 MB in 0s (69.2 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.17_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.17) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.17) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126875 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.17_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.17) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.71+ubuntu22.04_amd64.deb ...\n",
            "Unpacking snapd (2.71+ubuntu22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.17) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.71+ubuntu22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 127105 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.6_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.6) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.6) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.17) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем необходимые классы из selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# --- Настройка опций для Chrome ---\n",
        "# Создаем объект опций\n",
        "chrome_options = Options()\n",
        "\n",
        "# Эти опции КРАЙНЕ ВАЖНЫ для работы в Google Colab\n",
        "# 1. '--headless' — запускает браузер без графического интерфейса.\n",
        "chrome_options.add_argument('--headless')\n",
        "\n",
        "# 2. '--no-sandbox' — отключает \"песочницу\", что часто требуется для запуска в Docker-контейнерах, как в Colab.\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "\n",
        "# 3. '--disable-dev-shm-usage' — предотвращает проблемы с нехваткой памяти в некоторых окружениях.\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "print(\"Опции для запуска Chrome успешно настроены.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvB_cLsgbO3o",
        "outputId": "7339202d-c17e-4532-ec4f-4927f5bc07b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Опции для запуска Chrome успешно настроены.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "# --- Инициализация веб-драйвера ---\n",
        "# Создаем экземпляр драйвера Chrome, передавая ему наши настроенные опции\n",
        "# Selenium автоматически найдет chromedriver, установленный на предыдущем шаге\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "print(\"Веб-драйвер успешно запущен!\")\n",
        "\n",
        "# --- Выполнение задачи ---\n",
        "url = 'https://ru.wikipedia.org/wiki/Python'\n",
        "\n",
        "print(f\"Переходим на страницу: {url}\")\n",
        "# Открываем страницу в виртуальном браузере\n",
        "driver.get(url)\n",
        "\n",
        "# Даем странице пару секунд на полную прогрузку (хорошая практика)\n",
        "time.sleep(2)\n",
        "\n",
        "try:\n",
        "    # Находим элемент по его ID. Заголовок статьи в Википедии имеет id='firstHeading'\n",
        "    # By.ID — это стратегия поиска, указывающая, что мы ищем по идентификатору.\n",
        "    header_element = driver.find_element(By.ID, 'firstHeading')\n",
        "\n",
        "    # Получаем текстовое содержимое найденного элемента\n",
        "    header_text = header_element.text\n",
        "\n",
        "    print(\"\\n--- Результат ---\")\n",
        "    print(f\"Заголовок страницы: '{header_text}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Не удалось найти элемент. Ошибка: {e}\")\n",
        "\n",
        "finally:\n",
        "    # --- Завершение работы ---\n",
        "    # КРАЙНЕ ВАЖНО закрыть драйвер, чтобы освободить ресурсы\n",
        "    driver.quit()\n",
        "    print(\"\\nРабота драйвера завершена, ресурсы освобождены.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoi5X4jXbjjl",
        "outputId": "df758727-4a62-4591-a5fb-1c1b44367f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Веб-драйвер успешно запущен!\n",
            "Переходим на страницу: https://ru.wikipedia.org/wiki/Python\n",
            "\n",
            "--- Результат ---\n",
            "Заголовок страницы: 'Python'\n",
            "\n",
            "Работа драйвера завершена, ресурсы освобождены.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ЗАПУСК ДРАЙВЕРА ---\n",
        "driver = setup_driver()\n",
        "\n",
        "try:\n",
        "    # --- ШАГ 1: Открытие сайта ---\n",
        "    url = 'https://www.aviasales.ru/'\n",
        "    driver.get(url)\n",
        "    print(f\"Успешно перешел на страницу: {url}\")\n",
        "\n",
        "    # --- ШАГ 2: Принятие Cookie ---\n",
        "    print(\"Ожидаю появления баннера о cookie...\")\n",
        "    try:\n",
        "        # ЗАДАНИЕ 1: Найдите кнопку принятия cookie и нажмите на нее.\n",
        "        # Подсказка: у кнопки есть атрибут data-test-id=\"accept-cookies-button\".\n",
        "        # Используйте WebDriverWait, чтобы дождаться, пока кнопка станет кликабельной.\n",
        "        cookie_wait = WebDriverWait(driver, 10) # Ждем до 10 секунд\n",
        "\n",
        "        # ▼▼▼ НАПИШИТЕ ВАШ КОД ЗДЕСЬ (2 строки) ▼▼▼\n",
        "        accept_button = cookie_wait.until(\n",
        "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"...\")) # Вставьте сюда правильный CSS-селектор\n",
        "        )\n",
        "        ... # Нажмите на кнопку\n",
        "        # ▲▲▲ КОНЕЦ ВАШЕГО КОДА ▲▲▲\n",
        "\n",
        "        print(\"Баннер о cookie успешно принят.\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    except Exception:\n",
        "        print(\"Баннер о cookie не найден за 10 секунд, продолжаю выполнение.\")\n",
        "\n",
        "    # --- ШАГ 3: Заполнение формы поиска ---\n",
        "\n",
        "    # ЗАДАНИЕ 2: Введите \"Москва\" в поле \"Откуда\".\n",
        "    # Подсказка: найдите поле по его 'id'.\n",
        "    # ▼▼▼ НАПИШИТЕ ВАШ КОД ЗДЕСЬ (3 строки) ▼▼▼\n",
        "    origin_input = driver.find_element(By.ID, \"...\") # Вставьте ID поля \"Откуда\"\n",
        "    origin_input.clear()\n",
        "    origin_input.send_keys(\"...\") # Введите город\n",
        "    # ▲▲▲ КОНЕЦ ВАШЕГО КОДА ▲▲▲\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    # ЗАДАНИЕ 3: Введите \"Санкт-Петербург\" в поле \"Куда\".\n",
        "    # Подсказка: найдите поле по его 'id'.\n",
        "    # ▼▼▼ НАПИШИТЕ ВАШ КОД ЗДЕСЬ (3 строки) ▼▼▼\n",
        "    destination_input = driver.find_element(By.ID, \"...\") # Вставьте ID поля \"Куда\"\n",
        "    destination_input.clear()\n",
        "    destination_input.send_keys(\"...\") # Введите город\n",
        "    # ▲▲▲ КОНЕЦ ВАШЕГО КОДА ▲▲▲\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    # ЗАДАНИЕ 4: Нажмите на кнопку \"Найти билеты\".\n",
        "    # Подсказка: у кнопки есть удобный атрибут data-test-id=\"form-submit\".\n",
        "    # ▼▼▼ НАПИШИТЕ ВАШ КОД ЗДЕСЬ (2 строки) ▼▼▼\n",
        "    search_button = driver.find_element(By.CSS_SELECTOR, \"...\") # Вставьте CSS-селектор кнопки\n",
        "    search_button.click()\n",
        "    # ▲▲▲ КОНЕЦ ВАШЕГО КОДА ▲▲▲\n",
        "\n",
        "    # --- ШАГ 4: Ожидание и извлечение результатов ---\n",
        "    print(\"\\nФорма заполнена. Ожидаю загрузки билетов (может занять до 30 секунд)...\")\n",
        "\n",
        "    # ЗАДАНИЕ 5: Дождитесь появления первого билета.\n",
        "    # Подсказка: карточка билета имеет атрибут data-test-id=\"ticket-card\".\n",
        "    # Используйте WebDriverWait.\n",
        "    wait = WebDriverWait(driver, 30) # Увеличим ожидание до 30 секунд\n",
        "\n",
        "    # ▼▼▼ НАПИШИТЕ ВАШ КОД ЗДЕСЬ (3 строки) ▼▼▼\n",
        "    first_ticket = wait.until(\n",
        "        EC.presence_of_element_located((By.CSS_SELECTOR, \"...\")) # Вставьте CSS-селектор карточки билета\n",
        "    )\n",
        "    # ▲▲▲ КОНЕЦ ВАШЕГО КОДА ▲▲▲\n",
        "\n",
        "    print(\"Страница с результатами загружена!\")\n",
        "\n",
        "    # ЗАДАНИЕ 6: Извлеките цену из найденного билета.\n",
        "    # Подсказка: элемент с ценой находится ВНУТРИ 'first_ticket' и имеет атрибут data-test-id=\"price\".\n",
        "    # ▼▼▼ НАПИШИТЕ ВАШ КОД ЗДЕСЬ (2 строки) ▼▼▼\n",
        "    price_element = first_ticket.find_element(By.CSS_SELECTOR, \"...\") # Вставьте CSS-селектор цены\n",
        "    price_text = ... # Получите текст из элемента\n",
        "    # ▲▲▲ КОНЕЦ ВАШЕГО КОДА ▲▲▲\n",
        "\n",
        "    print(\"\\n--- РЕЗУЛЬТАТ ---\")\n",
        "    print(f\"Цена первого билета: {price_text}\")\n",
        "\n",
        "    driver.save_screenshot('aviasales_results.png')\n",
        "    print(\"Скриншот 'aviasales_results.png' сохранен. Вы можете посмотреть его в файлах слева.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ Произошла ошибка: {e}\")\n",
        "    driver.save_screenshot('aviasales_error.png')\n",
        "    print(\"Скриншот 'aviasales_error.png' сохранен для анализа ошибки.\")\n",
        "\n",
        "finally:\n",
        "    # --- ШАГ 5: Закрытие браузера (Обязательно!) ---\n",
        "    driver.quit()\n",
        "    print(\"\\nРабота драйвера завершена.\")"
      ],
      "metadata": {
        "id": "5pWFheblgDjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Контрольные вопросы по библиотекам для парсинга**\n",
        "\n",
        "#### **Блок 1: Библиотека `requests` (\"Курьер\")**\n",
        "\n",
        "Эти вопросы проверяют ваше умение получать данные с сервера.\n",
        "\n",
        "1.  **Фундаментальный вопрос:** В чем основная задача библиотеки `requests`? Что она делает, а чего, наоборот, делать **не умеет** (например, в контексте JavaScript)?\n",
        "2.  **Типы запросов:** В чем принципиальная разница между `GET` и `POST` запросами? В какой из наших лабораторных задач мы использовали `POST` и для какой цели?\n",
        "3.  **Объект ответа:** Вы выполнили команду `response = requests.get(url)`. Какие три важнейших атрибута объекта `response` вы будете использовать и для чего каждый из них предназначен (`status_code`, `text`, `content`)?\n",
        "4.  **Обработка ошибок:** Почему проверка `response.status_code == 200` является обязательным шагом в любом надежном парсере? Что означает код `404`? А код `403`?\n",
        "5.  **Работа с API:** Почему для получения данных от API (как в задаче с GitHub) мы использовали метод `response.json()`, а не просто брали `response.text`? В чем преимущество такого подхода?\n",
        "6.  **Сессии:** Объясните своими словами, что такое `requests.Session()`. Какую проблему решает объект сессии, и почему без него не удалось бы выполнить задачу с авторизацией на сайте?\n",
        "7.  **Параметры запроса:** Как с помощью `requests` передать в URL GET-параметры (например, `?sort=date&page=2`) без ручного формирования строки URL? Какой аргумент функции `get()` для этого используется?\n",
        "\n",
        "---\n",
        "\n",
        "#### **Блок 2: Библиотека `BeautifulSoup` (\"Навигатор\")**\n",
        "\n",
        "Эти вопросы проверяют ваше умение разбирать HTML-код и находить в нем нужные данные.\n",
        "\n",
        "8.  **Основное назначение:** Какую проблему решает `BeautifulSoup`? Что она принимает на вход и что отдает на выходе?\n",
        "9.  **Ключевое различие:** В чем фундаментальная разница между методами `.find()` и `.find_all()`? Приведите пример, когда нужно использовать один, а когда — другой.\n",
        "10. **Синтаксис поиска:** Как найти тег `<p>` с CSS-классом `price_color`? Почему в коде мы пишем `class_` с нижним подчеркиванием, а не просто `class`?\n",
        "11. **Извлечение данных:** У вас есть объект тега, сохраненный в переменной `tag`. Как из него извлечь:\n",
        "    *   Весь видимый текст внутри него?\n",
        "    *   Значение атрибута `href`?\n",
        "12. **Вложенный поиск:** Ваш парсер нашел общий контейнер товара (`<div class=\"product\">`). Как продолжить поиск и найти цену, которая находится **внутри** этого контейнера? Напишите примерный код.\n",
        "13. **Продвинутая навигация:** Представьте, что вы нашли заголовок `<h2>Описание</h2>`. Само описание находится в следующем за ним теге `<p>`. Какой метод `BeautifulSoup` позволит вам найти этот \"соседний\" тег, не начиная поиск заново от корня документа?\n",
        "14. **Очистка данных:** Почему простого извлечения `.text` часто недостаточно для реальных задач? Какие две стандартные операции по очистке текста вы применяли в лабораторных работах?\n",
        "\n",
        "---\n",
        "\n",
        "#### **Блок 3: Библиотека `Selenium` (\"Робот-пользователь\")**\n",
        "\n",
        "Эти вопросы проверяют ваше понимание работы с динамическими сайтами и автоматизацией браузера.\n",
        "\n",
        "15. **Главный вопрос:** Назовите основную причину, по которой мы вынуждены использовать `Selenium`, а не `requests`. Какую технологию `Selenium` умеет обрабатывать, а `requests` — нет?\n",
        "16. **Проблема синхронизации:** Почему использование `time.sleep()` для ожидания загрузки элементов на странице является плохой практикой? Каков правильный, надежный способ дождаться появления элемента?\n",
        "17. **Явные ожидания (Explicit Waits):** Объясните своими словами, что делают эти три строки кода:\n",
        "    ```python\n",
        "    wait = WebDriverWait(driver, 10)\n",
        "    element = wait.until(\n",
        "        EC.presence_of_element_located((By.CLASS_NAME, 'price'))\n",
        "    )\n",
        "    ```\n",
        "18. **Взаимодействие с формами:** Опишите последовательность из трех основных действий, которые нужно совершить с помощью `Selenium`, чтобы ввести текст в поле поиска и нажать на кнопку.\n",
        "19. **Исполнение JavaScript:** Для чего в задаче с \"бесконечным свитком\" мы использовали команду `driver.execute_script()`? Можно ли было добиться того же результата другим методом `Selenium`?\n",
        "20. **Интеграция библиотек:** В какой момент работы парсера на `Selenium` имеет смысл передать управление библиотеке `BeautifulSoup`? Что для этого нужно получить от `driver` и как это сделать?\n",
        "21. **Завершение работы:** Почему команда `driver.quit()` является обязательной в конце скрипта? Что произойдет, если ее не вызывать?\n",
        "\n",
        "---\n",
        "\n",
        "#### **Блок 4: Синтез и сценарии (Проверка общего понимания)**\n",
        "\n",
        "22. **Выбор инструмента:** Вам нужно спарсить три сайта:\n",
        "    *   А) Таблицу курсов валют со страницы Центробанка.\n",
        "    *   Б) Ленту комментариев на YouTube, которая подгружается при прокрутке.\n",
        "    *   В) Данные о погоде с публичного погодного API.\n",
        "    Какой основной инструмент (`requests`, `bs4`, `Selenium`) вы выберете для **каждой** из этих задач и почему?\n",
        "\n",
        "23. **Отладка:** Ваш парсер на `BeautifulSoup` вчера работал, а сегодня перестал, выдавая ошибку `AttributeError: 'NoneType' object has no attribute 'text'`. Назовите самую вероятную причину этой проблемы. Каков ваш первый шаг для диагностики?\n",
        "\n",
        "24. **Этика парсинга:** Что такое файл `robots.txt` на сайте и почему его рекомендуется проверять перед запуском массового сбора данных?"
      ],
      "metadata": {
        "id": "MorSgT4YmN_A"
      }
    }
  ]
}